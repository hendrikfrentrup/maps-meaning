{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types.{LongType, StringType, StructField, StructType}\n",
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.graphx.{Edge, Graph, PartitionID, VertexId}\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.{Column, DataFrame, Dataset, Row}\n",
    "// import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "record_schema = StructType(StructField(ip,StringType,true), StructField(mac,StringType,true), StructField(hostname,StringType,true), StructField(serial_no,StringType,true), StructField(owner,StringType,true), StructField(source_name,StringType,true), StructField(id,LongType,false))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "StructType(StructField(ip,StringType,true), StructField(mac,StringType,true), StructField(hostname,StringType,true), StructField(serial_no,StringType,true), StructField(owner,StringType,true), StructField(source_name,StringType,true), StructField(id,LongType,false))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val record_schema = StructType(Array(\n",
    "    StructField(\"ip\", StringType, nullable=true),\n",
    "    StructField(\"mac\", StringType, nullable=true),\n",
    "    StructField(\"hostname\", StringType, nullable=true),\n",
    "    StructField(\"serial_no\", StringType, nullable=true),\n",
    "    StructField(\"owner\", StringType, nullable=true),\n",
    "    StructField(\"source_name\", StringType, nullable=true),\n",
    "    StructField(\"id\", LongType, nullable=false)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecordId = id\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "buildVerticesRDD: (df: org.apache.spark.sql.DataFrame)org.apache.spark.rdd.RDD[(org.apache.spark.graphx.VertexId, org.apache.spark.graphx.PartitionID)]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "id"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val RecordId = \"id\"\n",
    "def buildVerticesRDD(df: DataFrame): RDD[(VertexId, PartitionID)] = {\n",
    "    def asVertexIdTuple(record: Row): (VertexId, PartitionID) = (record.getAs[VertexId](RecordId), 1)\n",
    "\n",
    "    df.map(asVertexIdTuple).rdd\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "records = [ip: string, mac: string ... 5 more fields]\n",
       "mirrorColNames = Array(_ip, _mac, _hostname, _serial_no, _owner, _source_name, _id)\n",
       "mirror = [_ip: string, _mac: string ... 5 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[_ip: string, _mac: string ... 5 more fields]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val records = spark.read.format(\"csv\")\n",
    "                .option(\"header\", \"true\")\n",
    "                .schema(record_schema)\n",
    "                .load(\"data/small_test_data.csv\")\n",
    "val mirrorColNames = for (col <- records.columns) yield \"_\"+col.toString\n",
    "// val mirrorColNames = mirror.columns.foreach(col => \"mirror_\"+col.toString)\n",
    "val mirror = records.toDF(mirrorColNames: _*)\n",
    "// .withColumnRenamed(\"record_id\", \"mirror_record_id\")\n",
    "// "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------------+--------------------+----------+----------------+------------+---+\n",
      "|           _ip|             _mac|           _hostname|_serial_no|          _owner|_source_name|_id|\n",
      "+--------------+-----------------+--------------------+----------+----------------+------------+---+\n",
      "|          null|29:0b:58:3d:26:6d|isoulsby3v@behanc...|KXHGN8D8KM|    Izzy Soulsby|          LD|  0|\n",
      "|          null|a9:11:e3:35:23:fa| cpollak74@nifty.com|Y5K30C65M3| Cathlene Pollak|          LD|  1|\n",
      "|          null|90:22:03:25:11:d8|  ckissock53@php.net|7ZGA85QA5H|   Colin Kissock|          LD|  2|\n",
      "|          null|4c:31:07:e9:9a:90|  bdruhan1m@1688.com|U0UOS9R43J|Brittaney Druhan|          LD|  3|\n",
      "|106.60.189.138|             null|  bdruhan1m@1688.com|      null|            null|          SY|  4|\n",
      "|106.60.189.138|4c:31:07:e9:9a:90|                null|      null|            null|          DT|  5|\n",
      "+--------------+-----------------+--------------------+----------+----------------+------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// records.printSchema\n",
    "mirror.show()\n",
    "// mirror.schema.foreach(e => println(e.name))\n",
    "// records.schema.foreach(e => println(e.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((NOT (id = _id)) AND ((ip = _ip) OR (mac = _mac)))\n",
      "[3,5]\n",
      "[4,5]\n",
      "[5,3]\n",
      "[5,4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "gggg = [ip: string, mac: string ... 12 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "cond: (matchCols: Seq[String])org.apache.spark.sql.Column\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ip: string, mac: string ... 12 more fields]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cond(matchCols: Seq[String]): Column = {\n",
    "    col(\"id\")=!=col(\"_id\") && matchCols.map(c => col(c)===col(\"_\"+c)).reduce(_ || _)\n",
    "//     \n",
    "//     col(\"id\")=!=col(\"_id\") && col(matchCol).equalTo(col(\"_\"+matchCol))\n",
    "}\n",
    "\n",
    "println(cond(Seq(\"ip\", \"mac\")))\n",
    "// println(cond(\"ip\"))\n",
    "\n",
    "val gggg = records.join(mirror, cond(Seq(\"ip\", \"mac\")))\n",
    "gggg.select(\"id\", \"_id\").collect.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[106.60.189.138,null,bdruhan1m@1688.com,null,null,SY,4,106.60.189.138,4c:31:07:e9:9a:90,null,null,null,DT,5]\n",
      "[106.60.189.138,4c:31:07:e9:9a:90,null,null,null,DT,5,106.60.189.138,null,bdruhan1m@1688.com,null,null,SY,4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "edges = [ip: string, mac: string ... 12 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ip: string, mac: string ... 12 more fields]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val edges = records.join(mirror, records(\"id\").notEqual(mirror(\"_id\")) && \n",
    "                                 records(\"ip\").equalTo(mirror(\"_ip\")) ||\n",
    "                                 records(\"mac\").equalTo(mirror(\"_mac\"))\n",
    "                        )\n",
    "//                      df1(\"record_id\") =!= df2(\"record_id\") )\n",
    "//                            && (df1(\"ip\") <=> df2(\"ip\"))) )\n",
    "// select($\"record_id\")\n",
    "edges.select(\"id\", \"_id\").collect.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+\n",
      "|record_id|mirror_record_id|\n",
      "+---------+----------------+\n",
      "|        3|               5|\n",
      "|        3|               4|\n",
      "|        4|               5|\n",
      "|        4|               3|\n",
      "|        5|               3|\n",
      "|        5|               4|\n",
      "+---------+----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "edges = [ip: string, mac: string ... 12 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ip: string, mac: string ... 12 more fields]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val edges = records.join(mirror).where($\"record_id\".notEqual($\"mirror_record_id\") && \n",
    "                                       ($\"ip\".equalTo($\"mirror_ip\") ||\n",
    "                                        $\"mac\".equalTo($\"mirror_mac\") ||\n",
    "                                        $\"hostname\".equalTo($\"mirror_hostname\")\n",
    "                                       ) \n",
    "                                      )\n",
    "edges.select(\"record_id\",\"mirror_record_id\").distinct.show()\n",
    "// .collect.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List(ip, mac)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "out = List(ip, mac)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "List(ip, mac)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val out = for (col <- Seq(\"ip\", \"mac\")) yield col.toString\n",
    "println(out.reduce())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "edges = [ip: string, mac: string ... 12 more fields]\n",
       "temp = [record_id: bigint, mirror_record_id: bigint]\n",
       "edgesRDD = MapPartitionsRDD[16] at rdd at <console>:60\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[16] at rdd at <console>:60"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val edges = records.join(mirror, ($\"record_id\"=!=$\"mirror_record_id\" && \n",
    "                                  $\"ip\"===$\"mirror_ip\")\n",
    "                        )\n",
    "val temp = edges.select(\"record_id\",\"mirror_record_id\").distinct\n",
    "// .show()\n",
    "\n",
    "// val asEdge: Row => Edge[PartitionID] = row => Edge[PartitionID](row.getAs[VertexId](0), row.getAs[VertexId](1), attr = 1)\n",
    "// val edgesRDD = edges.map(asEdge).rdd\n",
    "\n",
    "// val edgesDF = temp.map(edge => Edge[PartitionID](edge.getAs[VertexId](0), edge.getAs[VertexId](1), 1)).rdd\n",
    "val edgesRDD = temp.map(edge => Edge(edge.getAs[VertexId](0), edge.getAs[VertexId](1))).rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(Edge(4,5,null), Edge(5,4,null))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edgesRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "edgeRDD = MapPartitionsRDD[320] at rdd at <console>:49\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[320] at rdd at <console>:49"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val edgeRDD = edges.select(\"record_id\",\"mirror_record_id\").withColumn(\"edge_id\", monotonically_increasing_id).rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// val vertices: RDD[(VertexId, _)] =\n",
    "// def asVertexIdTuple(record: Row): (VertexId, PartitionID) = (record.getAs[VertexId](\"record_id\"), 1)\n",
    "records.withColumn(\"id\", monotonically_increasing_id).select(\"id\").map(r => r.getAs[Int](0)).rdd.collect()\n",
    "// .rdd.collect\n",
    "// .map(_.record_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ip: string, mac: string ... 5 more fields]"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// val nodes: RDD[(VertexId, (String, String))] = \n",
    "records\n",
    "// .map( r => r(0) )\n",
    "// .collect.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nodes = MapPartitionsRDD[97] at rdd at <console>:38\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[97] at rdd at <console>:38"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val nodes = buildVerticesRDD(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array((0,1), (1,1), (2,1), (3,1), (4,1), (5,1))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Unknown Error\n",
       "Message: <console>:46: error: type mismatch;\n",
       " found   : org.apache.spark.sql.Row\n",
       " required: TraversableOnce[?]\n",
       "       val nodes: RDD[(VertexId, Option[String])] = records.select(\"record_id\").flatMap(array => array).map((_.toLong, None))\n",
       "                                                                                                 ^\n",
       "<console>:46: error: missing parameter type for expanded function ((x$1) => x$1.toLong)\n",
       "       val nodes: RDD[(VertexId, Option[String])] = records.select(\"record_id\").flatMap(array => array).map((_.toLong, None))\n",
       "                                                                                                             ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val nodes: RDD[(VertexId, Option[String])] = records.select(\"record_id\").flatMap(array => array).map((_.toLong, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Unknown Error\n",
       "Message: <console>:48: error: type mismatch;\n",
       " found   : org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Nothing]]\n",
       " required: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[ED]]\n",
       "Error occurred in an application involving default arguments.\n",
       "       val graph = Graph(nodes, edgesRDD)\n",
       "                                ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val graph = Graph(nodes, edgesRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "users = ParallelCollectionRDD[450] at parallelize at <console>:61\n",
       "relationships = ParallelCollectionRDD[451] at parallelize at <console>:65\n",
       "graph = org.apache.spark.graphx.impl.GraphImpl@503343a8\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.graphx.impl.GraphImpl@503343a8"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val users: RDD[(VertexId, (String))] =\n",
    "  sc.parallelize(Array((3L, (\"rxin\")), (7L, (\"jgonzal\")),\n",
    "                       (5L, (\"franklin\")), (2L, (\"istoica\"))))\n",
    "// Create an RDD for edges\n",
    "val relationships: RDD[Edge[String]] =\n",
    "  sc.parallelize(Array(Edge(3L, 7L, \"match\"),    Edge(5L, 3L, \"match\"),\n",
    "                       Edge(2L, 5L, \"match\"), Edge(5L, 7L, \"match\")))\n",
    "// Define a default user in case there are relationship with missing user\n",
    "// val defaultUser = (\"John Doe\", \"Missing\")\n",
    "// Build the initial Graph\n",
    "val graph = Graph(users, relationships)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
