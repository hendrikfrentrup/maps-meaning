{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types.{LongType, StringType, StructField, StructType}\n",
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.{Column, DataFrame, Dataset, Row}\n",
    "// import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "record_schema = StructType(StructField(ip,StringType,true), StructField(mac,StringType,true), StructField(hostname,StringType,true), StructField(serial_no,StringType,true), StructField(owner,StringType,true), StructField(source_name,StringType,true), StructField(record_id,LongType,false))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "StructType(StructField(ip,StringType,true), StructField(mac,StringType,true), StructField(hostname,StringType,true), StructField(serial_no,StringType,true), StructField(owner,StringType,true), StructField(source_name,StringType,true), StructField(record_id,LongType,false))"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val record_schema = StructType(Array(\n",
    "    StructField(\"ip\", StringType, nullable=true),\n",
    "    StructField(\"mac\", StringType, nullable=true),\n",
    "    StructField(\"hostname\", StringType, nullable=true),\n",
    "    StructField(\"serial_no\", StringType, nullable=true),\n",
    "    StructField(\"owner\", StringType, nullable=true),\n",
    "    StructField(\"source_name\", StringType, nullable=true),\n",
    "    StructField(\"record_id\", LongType, nullable=false)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecordId = record_id\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "buildVerticesRDD: (df: org.apache.spark.sql.DataFrame)org.apache.spark.rdd.RDD[(org.apache.spark.graphx.VertexId, org.apache.spark.graphx.PartitionID)]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "record_id"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val RecordId = \"record_id\"\n",
    "def buildVerticesRDD(df: DataFrame): RDD[(VertexId, PartitionID)] = {\n",
    "    def asVertexIdTuple(record: Row): (VertexId, PartitionID) = (record.getAs[VertexId](RecordId), 1)\n",
    "\n",
    "    df.map(asVertexIdTuple).rdd\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "records = [ip: string, mac: string ... 5 more fields]\n",
       "mirrorColNames = Array(mirror_ip, mirror_mac, mirror_hostname, mirror_serial_no, mirror_owner, mirror_source_name, mirror_record_id)\n",
       "mirror = [mirror_ip: string, mirror_mac: string ... 5 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[mirror_ip: string, mirror_mac: string ... 5 more fields]"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val records = spark.read.format(\"csv\")\n",
    "                .option(\"header\", \"true\")\n",
    "                .schema(record_schema)\n",
    "                .load(\"data/small_test_data.csv\")\n",
    "val mirrorColNames = for (col <- records.columns) yield \"mirror_\"+col.toString\n",
    "// val mirrorColNames = mirror.columns.foreach(col => \"mirror_\"+col.toString)\n",
    "val mirror = records.toDF(mirrorColNames: _*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(StructField(ip,StringType,true), StructField(mac,StringType,true), StructField(hostname,StringType,true), StructField(serial_no,StringType,true), StructField(owner,StringType,true), StructField(source_name,StringType,true), StructField(record_id,LongType,true))"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// records.printSchema\n",
    "// records.show()\n",
    "records.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[106.60.189.138,null,bdruhan1m@1688.com,null,null,SY,4,106.60.189.138,4c:31:07:e9:9a:90,null,null,null,DT,5]\n",
      "[106.60.189.138,4c:31:07:e9:9a:90,null,null,null,DT,5,106.60.189.138,null,bdruhan1m@1688.com,null,null,SY,4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "edges = [ip: string, mac: string ... 12 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ip: string, mac: string ... 12 more fields]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val edges = df1.join(df2,   df1(\"record_id\").notEqual(df2(\"record_id\")) && \n",
    "                            df1(\"ip\").equalTo(df2(\"ip\")) )\n",
    "//                      df1(\"record_id\") =!= df2(\"record_id\") )\n",
    "//                            && (df1(\"ip\") <=> df2(\"ip\"))) )\n",
    "// select($\"record_id\")\n",
    "edges.collect.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+\n",
      "|record_id|mirror_record_id|\n",
      "+---------+----------------+\n",
      "|        5|               4|\n",
      "|        5|               3|\n",
      "|        3|               5|\n",
      "|        3|               4|\n",
      "|        4|               5|\n",
      "|        4|               3|\n",
      "+---------+----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "edges = [ip: string, mac: string ... 12 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ip: string, mac: string ... 12 more fields]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val edges = records.join(mirror).where($\"record_id\".notEqual($\"mirror_record_id\") && \n",
    "                                       ($\"ip\".equalTo($\"mirror_ip\") ||\n",
    "                                        $\"mac\".equalTo($\"mirror_mac\") ||\n",
    "                                        $\"hostname\".equalTo($\"mirror_hostname\")\n",
    "                                       ) \n",
    "                                      )\n",
    "edges.select(\"record_id\",\"mirror_record_id\").distinct.show()\n",
    "// .collect.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+\n",
      "|record_id|mirror_record_id|\n",
      "+---------+----------------+\n",
      "|        4|               5|\n",
      "|        5|               4|\n",
      "+---------+----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "edges = [ip: string, mac: string ... 12 more fields]\n",
       "asEdge = > org.apache.spark.graphx.Edge[org.apache.spark.graphx.PartitionID] = <function1>\n",
       "edgesRDD = MapPartitionsRDD[558] at rdd at <console>:75\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[558] at rdd at <console>:75"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val edges = records.join(mirror, ($\"record_id\"=!=$\"mirror_record_id\" && \n",
    "                                  $\"ip\"===$\"mirror_ip\")\n",
    "                        )\n",
    "edges.select(\"record_id\",\"mirror_record_id\").distinct.show()\n",
    "\n",
    "val asEdge: Row => Edge[PartitionID] = row => Edge[PartitionID](row.getAs[VertexId](0), row.getAs[VertexId](1), attr = 1)\n",
    "val edgesRDD = edges.map(asEdge).rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Job aborted due to stage failure: Task 0 in stage 188.0 failed 1 times, most recent failure: Lost task 0.0 in stage 188.0 (TID 1525, localhost, executor driver): java.lang.ClassCastException\n",
       "\n",
       "Driver stacktrace:\n",
       "StackTrace: Driver stacktrace:\n",
       "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n",
       "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
       "  at scala.Option.foreach(Option.scala:257)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n",
       "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
       "  at org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n",
       "  ... 64 elided"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edgesRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "edgeRDD = MapPartitionsRDD[320] at rdd at <console>:49\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[320] at rdd at <console>:49"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val edgeRDD = edges.select(\"record_id\",\"mirror_record_id\").withColumn(\"edge_id\", monotonically_increasing_id).rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// val vertices: RDD[(VertexId, _)] =\n",
    "// def asVertexIdTuple(record: Row): (VertexId, PartitionID) = (record.getAs[VertexId](\"record_id\"), 1)\n",
    "records.withColumn(\"id\", monotonically_increasing_id).select(\"id\").map(r => r.getAs[Int](0)).rdd.collect()\n",
    "// .rdd.collect\n",
    "// .map(_.record_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ip: string, mac: string ... 5 more fields]"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// val nodes: RDD[(VertexId, (String, String))] = \n",
    "records\n",
    "// .map( r => r(0) )\n",
    "// .collect.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nodes = MapPartitionsRDD[540] at rdd at <console>:72\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[540] at rdd at <console>:72"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val nodes = buildVerticesRDD(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array((0,1), (1,1), (2,1), (3,1), (4,1), (5,1))"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val nodes: RDD[(VertexId, Option[String])] = records.select(\"record_id\").flatMap(array => array).map((_.toLong, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Unknown Error\n",
       "Message: <console>:51: error: type mismatch;\n",
       " found   : org.apache.spark.rdd.RDD[org.apache.spark.sql.Row]\n",
       " required: org.apache.spark.rdd.RDD[(org.apache.spark.graphx.VertexId, ?)]\n",
       "    (which expands to)  org.apache.spark.rdd.RDD[(Long, ?)]\n",
       "Error occurred in an application involving default arguments.\n",
       "       val graph = Graph(vertices.rdd, edgeRDD)\n",
       "                                  ^\n",
       "<console>:51: error: type mismatch;\n",
       " found   : org.apache.spark.rdd.RDD[org.apache.spark.sql.Row]\n",
       " required: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[?]]\n",
       "Error occurred in an application involving default arguments.\n",
       "       val graph = Graph(vertices.rdd, edgeRDD)\n",
       "                                       ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val graph = Graph(vertices.rdd, edgeRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "users = ParallelCollectionRDD[450] at parallelize at <console>:61\n",
       "relationships = ParallelCollectionRDD[451] at parallelize at <console>:65\n",
       "graph = org.apache.spark.graphx.impl.GraphImpl@503343a8\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.graphx.impl.GraphImpl@503343a8"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val users: RDD[(VertexId, (String))] =\n",
    "  sc.parallelize(Array((3L, (\"rxin\")), (7L, (\"jgonzal\")),\n",
    "                       (5L, (\"franklin\")), (2L, (\"istoica\"))))\n",
    "// Create an RDD for edges\n",
    "val relationships: RDD[Edge[String]] =\n",
    "  sc.parallelize(Array(Edge(3L, 7L, \"match\"),    Edge(5L, 3L, \"match\"),\n",
    "                       Edge(2L, 5L, \"match\"), Edge(5L, 7L, \"match\")))\n",
    "// Define a default user in case there are relationship with missing user\n",
    "// val defaultUser = (\"John Doe\", \"Missing\")\n",
    "// Build the initial Graph\n",
    "val graph = Graph(users, relationships)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
